<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Mark Otto, Jacob Thornton, and Bootstrap contributors">
    <meta name="generator" content="Hugo 0.101.0">
    <title>Pietro Bongini</title>

    <link rel="canonical" href="https://getbootstrap.com/docs/5.2/examples/album/">

    <!-- CSS only -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-gH2yIJqKdNHPEq0n4Mqa/HGKIhSkIHeL5AyhkYV8i59U5AR6csBvApHHNl/vI1Bx" crossorigin="anonymous">
    <!-- JavaScript Bundle with Popper -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0/dist/js/bootstrap.bundle.min.js" integrity="sha384-A3rJD856KowSb7dwlZdYEkO39Gagi7vIsF0jrRAoQmDKKtQBHUuLZ9AsSv4jD4Xa" crossorigin="anonymous"></script>
    <script src="extend.js"></script>

<link href="../assets/dist/css/bootstrap.min.css" rel="stylesheet">

    <style>
      .bd-placeholder-img {
        font-size: 1.125rem;
        text-anchor: middle;
        -webkit-user-select: none;
        -moz-user-select: none;
        user-select: none;
      }

      @media (min-width: 768px) {
        .bd-placeholder-img-lg {
          font-size: 3.5rem;
        }
      }

      .b-example-divider {
        height: 3rem;
        background-color: rgba(0, 0, 0, .1);
        border: solid rgba(0, 0, 0, .15);
        border-width: 1px 0;
        box-shadow: inset 0 .5em 1.5em rgba(0, 0, 0, .1), inset 0 .125em .5em rgba(0, 0, 0, .15);
      }

      .b-example-vr {
        flex-shrink: 0;
        width: 1.5rem;
        height: 100vh;
      }

      .bi {
        vertical-align: -.125em;
        fill: currentColor;
      }

      .nav-scroller {
        position: relative;
        z-index: 2;
        height: 2.75rem;
        overflow-y: hidden;
      }

      .nav-scroller .nav {
        display: flex;
        flex-wrap: nowrap;
        padding-bottom: 1rem;
        margin-top: -1px;
        overflow-x: auto;
        text-align: center;
        white-space: nowrap;
        -webkit-overflow-scrolling: touch;
      }
      
     
    </style>

    
  </head>
  <body>
    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="bootstrap" viewBox="0 0 118 94">
    <title>Bootstrap</title>
    <path fill-rule="evenodd" clip-rule="evenodd" d="M24.509 0c-6.733 0-11.715 5.893-11.492 12.284.214 6.14-.064 14.092-2.066 20.577C8.943 39.365 5.547 43.485 0 44.014v5.972c5.547.529 8.943 4.649 10.951 11.153 2.002 6.485 2.28 14.437 2.066 20.577C12.794 88.106 17.776 94 24.51 94H93.5c6.733 0 11.714-5.893 11.491-12.284-.214-6.14.064-14.092 2.066-20.577 2.009-6.504 5.396-10.624 10.943-11.153v-5.972c-5.547-.529-8.934-4.649-10.943-11.153-2.002-6.484-2.28-14.437-2.066-20.577C105.214 5.894 100.233 0 93.5 0H24.508zM80 57.863C80 66.663 73.436 72 62.543 72H44a2 2 0 01-2-2V24a2 2 0 012-2h18.437c9.083 0 15.044 4.92 15.044 12.474 0 5.302-4.01 10.049-9.119 10.88v.277C75.317 46.394 80 51.21 80 57.863zM60.521 28.34H49.948v14.934h8.905c6.884 0 10.68-2.772 10.68-7.727 0-4.643-3.264-7.207-9.012-7.207zM49.948 49.2v16.458H60.91c7.167 0 10.964-2.876 10.964-8.281 0-5.406-3.903-8.178-11.425-8.178H49.948z"></path>
  </symbol>
  <symbol id="earmark" viewBox="0 0 16 16"> 
    <path d="M5.5 7a.5.5 0 0 0 0 1h5a.5.5 0 0 0 0-1h-5zM5 9.5a.5.5 0 0 1 .5-.5h5a.5.5 0 0 1 0 1h-5a.5.5 0 0 1-.5-.5zm0 2a.5.5 0 0 1 .5-.5h2a.5.5 0 0 1 0 1h-2a.5.5 0 0 1-.5-.5z"/>
  <path d="M9.5 0H4a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V4.5L9.5 0zm0 1v2A1.5 1.5 0 0 0 11 4.5h2V14a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1h5.5z"/>
 </symbol>
  <symbol id="person" viewBox="0 0 16 16"> 
  <path d="M8 8a3 3 0 1 0 0-6 3 3 0 0 0 0 6zm2-3a2 2 0 1 1-4 0 2 2 0 0 1 4 0zm4 8c0 1-1 1-1 1H3s-1 0-1-1 1-4 6-4 6 3 6 4zm-1-.004c-.001-.246-.154-.986-.832-1.664C11.516 10.68 10.289 10 8 10c-2.29 0-3.516.68-4.168 1.332-.678.678-.83 1.418-.832 1.664h10z"/>
  </symbol>
  <symbol id="activity" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M6 2a.5.5 0 0 1 .47.33L10 12.036l1.53-4.208A.5.5 0 0 1 12 7.5h3.5a.5.5 0 0 1 0 1h-3.15l-1.88 5.17a.5.5 0 0 1-.94 0L6 3.964 4.47 8.171A.5.5 0 0 1 4 8.5H.5a.5.5 0 0 1 0-1h3.15l1.88-5.17A.5.5 0 0 1 6 2Z"/>
  </symbol>
  <symbol id="home" viewBox="0 0 16 16">
    <path d="M8.354 1.146a.5.5 0 0 0-.708 0l-6 6A.5.5 0 0 0 1.5 7.5v7a.5.5 0 0 0 .5.5h4.5a.5.5 0 0 0 .5-.5v-4h2v4a.5.5 0 0 0 .5.5H14a.5.5 0 0 0 .5-.5v-7a.5.5 0 0 0-.146-.354L13 5.793V2.5a.5.5 0 0 0-.5-.5h-1a.5.5 0 0 0-.5.5v1.293L8.354 1.146zM2.5 14V7.707l5.5-5.5 5.5 5.5V14H10v-4a.5.5 0 0 0-.5-.5h-3a.5.5 0 0 0-.5.5v4H2.5z"/>
  </symbol>
  <symbol id="folder" viewBox="0 0 16 16">
  <path d="M1 3.5A1.5 1.5 0 0 1 2.5 2h2.764c.958 0 1.76.56 2.311 1.184C7.985 3.648 8.48 4 9 4h4.5A1.5 1.5 0 0 1 15 5.5v7a1.5 1.5 0 0 1-1.5 1.5h-11A1.5 1.5 0 0 1 1 12.5v-9zM2.5 3a.5.5 0 0 0-.5.5V6h12v-.5a.5.5 0 0 0-.5-.5H9c-.964 0-1.71-.629-2.174-1.154C6.374 3.334 5.82 3 5.264 3H2.5zM14 7H2v5.5a.5.5 0 0 0 .5.5h11a.5.5 0 0 0 .5-.5V7z"/>
  </symbol>
  <symbol id="speedometer2" viewBox="0 0 16 16">
    <path d="M8 4a.5.5 0 0 1 .5.5V6a.5.5 0 0 1-1 0V4.5A.5.5 0 0 1 8 4zM3.732 5.732a.5.5 0 0 1 .707 0l.915.914a.5.5 0 1 1-.708.708l-.914-.915a.5.5 0 0 1 0-.707zM2 10a.5.5 0 0 1 .5-.5h1.586a.5.5 0 0 1 0 1H2.5A.5.5 0 0 1 2 10zm9.5 0a.5.5 0 0 1 .5-.5h1.5a.5.5 0 0 1 0 1H12a.5.5 0 0 1-.5-.5zm.754-4.246a.389.389 0 0 0-.527-.02L7.547 9.31a.91.91 0 1 0 1.302 1.258l3.434-4.297a.389.389 0 0 0-.029-.518z"/>
    <path fill-rule="evenodd" d="M0 10a8 8 0 1 1 15.547 2.661c-.442 1.253-1.845 1.602-2.932 1.25C11.309 13.488 9.475 13 8 13c-1.474 0-3.31.488-4.615.911-1.087.352-2.49.003-2.932-1.25A7.988 7.988 0 0 1 0 10zm8-7a7 7 0 0 0-6.603 9.329c.203.575.923.876 1.68.63C4.397 12.533 6.358 12 8 12s3.604.532 4.923.96c.757.245 1.477-.056 1.68-.631A7 7 0 0 0 8 3z"/>
  </symbol>
  <symbol id="table" viewBox="0 0 16 16">
    <path d="M0 2a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v12a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V2zm15 2h-4v3h4V4zm0 4h-4v3h4V8zm0 4h-4v3h3a1 1 0 0 0 1-1v-2zm-5 3v-3H6v3h4zm-5 0v-3H1v2a1 1 0 0 0 1 1h3zm-4-4h4V8H1v3zm0-4h4V4H1v3zm5-3v3h4V4H6zm4 4H6v3h4V8z"/>
  </symbol>
  <symbol id="people-circle" viewBox="0 0 16 16">
    <path d="M11 6a3 3 0 1 1-6 0 3 3 0 0 1 6 0z"/>
    <path fill-rule="evenodd" d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8zm8-7a7 7 0 0 0-5.468 11.37C3.242 11.226 4.805 10 8 10s4.757 1.225 5.468 2.37A7 7 0 0 0 8 1z"/>
  </symbol>
  <symbol id="grid" viewBox="0 0 16 16">
    <path d="M1 2.5A1.5 1.5 0 0 1 2.5 1h3A1.5 1.5 0 0 1 7 2.5v3A1.5 1.5 0 0 1 5.5 7h-3A1.5 1.5 0 0 1 1 5.5v-3zM2.5 2a.5.5 0 0 0-.5.5v3a.5.5 0 0 0 .5.5h3a.5.5 0 0 0 .5-.5v-3a.5.5 0 0 0-.5-.5h-3zm6.5.5A1.5 1.5 0 0 1 10.5 1h3A1.5 1.5 0 0 1 15 2.5v3A1.5 1.5 0 0 1 13.5 7h-3A1.5 1.5 0 0 1 9 5.5v-3zm1.5-.5a.5.5 0 0 0-.5.5v3a.5.5 0 0 0 .5.5h3a.5.5 0 0 0 .5-.5v-3a.5.5 0 0 0-.5-.5h-3zM1 10.5A1.5 1.5 0 0 1 2.5 9h3A1.5 1.5 0 0 1 7 10.5v3A1.5 1.5 0 0 1 5.5 15h-3A1.5 1.5 0 0 1 1 13.5v-3zm1.5-.5a.5.5 0 0 0-.5.5v3a.5.5 0 0 0 .5.5h3a.5.5 0 0 0 .5-.5v-3a.5.5 0 0 0-.5-.5h-3zm6.5.5A1.5 1.5 0 0 1 10.5 9h3a1.5 1.5 0 0 1 1.5 1.5v3a1.5 1.5 0 0 1-1.5 1.5h-3A1.5 1.5 0 0 1 9 13.5v-3zm1.5-.5a.5.5 0 0 0-.5.5v3a.5.5 0 0 0 .5.5h3a.5.5 0 0 0 .5-.5v-3a.5.5 0 0 0-.5-.5h-3z"/>
  </symbol>
  <symbol id="collection" viewBox="0 0 16 16">
    <path d="M2.5 3.5a.5.5 0 0 1 0-1h11a.5.5 0 0 1 0 1h-11zm2-2a.5.5 0 0 1 0-1h7a.5.5 0 0 1 0 1h-7zM0 13a1.5 1.5 0 0 0 1.5 1.5h13A1.5 1.5 0 0 0 16 13V6a1.5 1.5 0 0 0-1.5-1.5h-13A1.5 1.5 0 0 0 0 6v7zm1.5.5A.5.5 0 0 1 1 13V6a.5.5 0 0 1 .5-.5h13a.5.5 0 0 1 .5.5v7a.5.5 0 0 1-.5.5h-13z"/>
  </symbol>
  <symbol id="calendar3" viewBox="0 0 16 16">
    <path d="M14 0H2a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2zM1 3.857C1 3.384 1.448 3 2 3h12c.552 0 1 .384 1 .857v10.286c0 .473-.448.857-1 .857H2c-.552 0-1-.384-1-.857V3.857z"/>
    <path d="M6.5 7a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm-9 3a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm-9 3a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2z"/>
  </symbol>
  <symbol id="chat-quote-fill" viewBox="0 0 16 16">
    <path d="M16 8c0 3.866-3.582 7-8 7a9.06 9.06 0 0 1-2.347-.306c-.584.296-1.925.864-4.181 1.234-.2.032-.352-.176-.273-.362.354-.836.674-1.95.77-2.966C.744 11.37 0 9.76 0 8c0-3.866 3.582-7 8-7s8 3.134 8 7zM7.194 6.766a1.688 1.688 0 0 0-.227-.272 1.467 1.467 0 0 0-.469-.324l-.008-.004A1.785 1.785 0 0 0 5.734 6C4.776 6 4 6.746 4 7.667c0 .92.776 1.666 1.734 1.666.343 0 .662-.095.931-.26-.137.389-.39.804-.81 1.22a.405.405 0 0 0 .011.59c.173.16.447.155.614-.01 1.334-1.329 1.37-2.758.941-3.706a2.461 2.461 0 0 0-.227-.4zM11 9.073c-.136.389-.39.804-.81 1.22a.405.405 0 0 0 .012.59c.172.16.446.155.613-.01 1.334-1.329 1.37-2.758.942-3.706a2.466 2.466 0 0 0-.228-.4 1.686 1.686 0 0 0-.227-.273 1.466 1.466 0 0 0-.469-.324l-.008-.004A1.785 1.785 0 0 0 10.07 6c-.957 0-1.734.746-1.734 1.667 0 .92.777 1.666 1.734 1.666.343 0 .662-.095.931-.26z"/>
  </symbol>
  <symbol id="cpu-fill" viewBox="0 0 16 16">
    <path d="M6.5 6a.5.5 0 0 0-.5.5v3a.5.5 0 0 0 .5.5h3a.5.5 0 0 0 .5-.5v-3a.5.5 0 0 0-.5-.5h-3z"/>
    <path d="M5.5.5a.5.5 0 0 0-1 0V2A2.5 2.5 0 0 0 2 4.5H.5a.5.5 0 0 0 0 1H2v1H.5a.5.5 0 0 0 0 1H2v1H.5a.5.5 0 0 0 0 1H2v1H.5a.5.5 0 0 0 0 1H2A2.5 2.5 0 0 0 4.5 14v1.5a.5.5 0 0 0 1 0V14h1v1.5a.5.5 0 0 0 1 0V14h1v1.5a.5.5 0 0 0 1 0V14h1v1.5a.5.5 0 0 0 1 0V14a2.5 2.5 0 0 0 2.5-2.5h1.5a.5.5 0 0 0 0-1H14v-1h1.5a.5.5 0 0 0 0-1H14v-1h1.5a.5.5 0 0 0 0-1H14v-1h1.5a.5.5 0 0 0 0-1H14A2.5 2.5 0 0 0 11.5 2V.5a.5.5 0 0 0-1 0V2h-1V.5a.5.5 0 0 0-1 0V2h-1V.5a.5.5 0 0 0-1 0V2h-1V.5zm1 4.5h3A1.5 1.5 0 0 1 11 6.5v3A1.5 1.5 0 0 1 9.5 11h-3A1.5 1.5 0 0 1 5 9.5v-3A1.5 1.5 0 0 1 6.5 5z"/>
  </symbol>
  <symbol id="gear-fill" viewBox="0 0 16 16">
    <path d="M9.405 1.05c-.413-1.4-2.397-1.4-2.81 0l-.1.34a1.464 1.464 0 0 1-2.105.872l-.31-.17c-1.283-.698-2.686.705-1.987 1.987l.169.311c.446.82.023 1.841-.872 2.105l-.34.1c-1.4.413-1.4 2.397 0 2.81l.34.1a1.464 1.464 0 0 1 .872 2.105l-.17.31c-.698 1.283.705 2.686 1.987 1.987l.311-.169a1.464 1.464 0 0 1 2.105.872l.1.34c.413 1.4 2.397 1.4 2.81 0l.1-.34a1.464 1.464 0 0 1 2.105-.872l.31.17c1.283.698 2.686-.705 1.987-1.987l-.169-.311a1.464 1.464 0 0 1 .872-2.105l.34-.1c1.4-.413 1.4-2.397 0-2.81l-.34-.1a1.464 1.464 0 0 1-.872-2.105l.17-.31c.698-1.283-.705-2.686-1.987-1.987l-.311.169a1.464 1.464 0 0 1-2.105-.872l-.1-.34zM8 10.93a2.929 2.929 0 1 1 0-5.86 2.929 2.929 0 0 1 0 5.858z"/>
  </symbol>
  <symbol id="speedometer" viewBox="0 0 16 16">
    <path d="M8 2a.5.5 0 0 1 .5.5V4a.5.5 0 0 1-1 0V2.5A.5.5 0 0 1 8 2zM3.732 3.732a.5.5 0 0 1 .707 0l.915.914a.5.5 0 1 1-.708.708l-.914-.915a.5.5 0 0 1 0-.707zM2 8a.5.5 0 0 1 .5-.5h1.586a.5.5 0 0 1 0 1H2.5A.5.5 0 0 1 2 8zm9.5 0a.5.5 0 0 1 .5-.5h1.5a.5.5 0 0 1 0 1H12a.5.5 0 0 1-.5-.5zm.754-4.246a.389.389 0 0 0-.527-.02L7.547 7.31A.91.91 0 1 0 8.85 8.569l3.434-4.297a.389.389 0 0 0-.029-.518z"/>
    <path fill-rule="evenodd" d="M6.664 15.889A8 8 0 1 1 9.336.11a8 8 0 0 1-2.672 15.78zm-4.665-4.283A11.945 11.945 0 0 1 8 10c2.186 0 4.236.585 6.001 1.606a7 7 0 1 0-12.002 0z"/>
  </symbol>
  <symbol id="toggles2" viewBox="0 0 16 16">
    <path d="M9.465 10H12a2 2 0 1 1 0 4H9.465c.34-.588.535-1.271.535-2 0-.729-.195-1.412-.535-2z"/>
    <path d="M6 15a3 3 0 1 0 0-6 3 3 0 0 0 0 6zm0 1a4 4 0 1 1 0-8 4 4 0 0 1 0 8zm.535-10a3.975 3.975 0 0 1-.409-1H4a1 1 0 0 1 0-2h2.126c.091-.355.23-.69.41-1H4a2 2 0 1 0 0 4h2.535z"/>
    <path d="M14 4a4 4 0 1 1-8 0 4 4 0 0 1 8 0z"/>
  </symbol>
  <symbol id="tools" viewBox="0 0 16 16">
    <path d="M1 0L0 1l2.2 3.081a1 1 0 0 0 .815.419h.07a1 1 0 0 1 .708.293l2.675 2.675-2.617 2.654A3.003 3.003 0 0 0 0 13a3 3 0 1 0 5.878-.851l2.654-2.617.968.968-.305.914a1 1 0 0 0 .242 1.023l3.356 3.356a1 1 0 0 0 1.414 0l1.586-1.586a1 1 0 0 0 0-1.414l-3.356-3.356a1 1 0 0 0-1.023-.242L10.5 9.5l-.96-.96 2.68-2.643A3.005 3.005 0 0 0 16 3c0-.269-.035-.53-.102-.777l-2.14 2.141L12 4l-.364-1.757L13.777.102a3 3 0 0 0-3.675 3.68L7.462 6.46 4.793 3.793a1 1 0 0 1-.293-.707v-.071a1 1 0 0 0-.419-.814L1 0zm9.646 10.646a.5.5 0 0 1 .708 0l3 3a.5.5 0 0 1-.708.708l-3-3a.5.5 0 0 1 0-.708zM3 11l.471.242.529.026.287.445.445.287.026.529L5 13l-.242.471-.026.529-.445.287-.287.445-.529.026L3 15l-.471-.242L2 14.732l-.287-.445L1.268 14l-.026-.529L1 13l.242-.471.026-.529.445-.287.287-.445.529-.026L3 11z"/>
  </symbol>
  <symbol id="chevron-right" viewBox="0 0 16 16">
    <path fill-rule="evenodd" d="M4.646 1.646a.5.5 0 0 1 .708 0l6 6a.5.5 0 0 1 0 .708l-6 6a.5.5 0 0 1-.708-.708L10.293 8 4.646 2.354a.5.5 0 0 1 0-.708z"/>
  </symbol>
  <symbol id="geo-fill" viewBox="0 0 16 16">
    <path fill-rule="evenodd" d="M4 4a4 4 0 1 1 4.5 3.969V13.5a.5.5 0 0 1-1 0V7.97A4 4 0 0 1 4 3.999zm2.493 8.574a.5.5 0 0 1-.411.575c-.712.118-1.28.295-1.655.493a1.319 1.319 0 0 0-.37.265.301.301 0 0 0-.057.09V14l.002.008a.147.147 0 0 0 .016.033.617.617 0 0 0 .145.15c.165.13.435.27.813.395.751.25 1.82.414 3.024.414s2.273-.163 3.024-.414c.378-.126.648-.265.813-.395a.619.619 0 0 0 .146-.15.148.148 0 0 0 .015-.033L12 14v-.004a.301.301 0 0 0-.057-.09 1.318 1.318 0 0 0-.37-.264c-.376-.198-.943-.375-1.655-.493a.5.5 0 1 1 .164-.986c.77.127 1.452.328 1.957.594C12.5 13 13 13.4 13 14c0 .426-.26.752-.544.977-.29.228-.68.413-1.116.558-.878.293-2.059.465-3.34.465-1.281 0-2.462-.172-3.34-.465-.436-.145-.826-.33-1.116-.558C3.26 14.752 3 14.426 3 14c0-.599.5-1 .961-1.243.505-.266 1.187-.467 1.957-.594a.5.5 0 0 1 .575.411z"/>
  </symbol>
  <symbol id="email" viewBox="0 0 16 16">
  <path d="M0 4a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V4Zm2-1a1 1 0 0 0-1 1v.217l7 4.2 7-4.2V4a1 1 0 0 0-1-1H2Zm13 2.383-4.708 2.825L15 11.105V5.383Zm-.034 6.876-5.64-3.471L8 9.583l-1.326-.795-5.64 3.47A1 1 0 0 0 2 13h12a1 1 0 0 0 .966-.741ZM1 11.105l4.708-2.897L1 5.383v5.722Z"/>
  </symbol>
  <symbol id="linkedin" viewBox="0 0 16 16">
  <path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54 0 0 1 .016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225h2.4z"/>
  </symbol>
</svg>
<header class="sticky-top" style="background-color:white">
   
    
    <section class="py-1 text-center container" style="background-image:url('./images/mare-2_1.jpg')">
        
            <div class="row py-lg-2 bg-image" >
                <div>
                    <img class="rounded-circle" alt="100x100" src="./images/foto.jpg"
                    data-holder-rendered="true" style='opacity:1'>
                </div>
                
                <div class="col-lg-6 col-md-8 mx-auto">
                    <div>
                        <h2 class="fw-light">Pietro Bongini</h2>
                        <p class="lead text-muted"> AI Researcher</p>
        <!--<p>
          <a href="#" class="btn btn-primary my-2">Main call to action</a>
          <a href="#" class="btn btn-secondary my-2">Secondary action</a>
        </p> -->
                    </div>
     
                </div>
            </div>
        
  </section>
    <div class="d-flex justify-content-around">
    <nav>
  <ul class="nav nav-pills">
    <li class="nav-item">
      <a href="#personal" class="nav-link active py-3 rounded-0" aria-current="page" title="Home" data-bs-toggle="tooltip" data-bs-placement="right">
          <svg class="bi pe-none" width="24" height="24" role="img" aria-label="Who I am"><use xlink:href="#person"/></svg>
        </a>
    </li>
    <li class="nav-item">
     <a href="#activity" class="nav-link py-3  rounded-0" title="Activity" data-bs-toggle="tooltip" data-bs-placement="right">
          <svg class="bi pe-none" width="24" height="24" role="img" aria-label="Activity"><use xlink:href="#activity"/></svg>
        </a>
    </li>
    <li>
        <a href="#resume" class="nav-link py-3  rounded-0" title="Publications" data-bs-toggle="tooltip" data-bs-placement="right">
          <svg class="bi pe-none" width="24" height="24" role="img" aria-label="Dashboard"><use xlink:href="#earmark"/></svg>
        </a>
      </li>
      <!--<li>
        <a href="#projects" class="nav-link py-3  rounded-0" title="Orders" data-bs-toggle="tooltip" data-bs-placement="right">
          <svg class="bi pe-none" width="24" height="24" role="img" aria-label="Orders"><use xlink:href="#folder"/></svg>
        </a>
      </li>-->
      <li>
        <a href="#contacts" class="nav-link py-3  rounded-0" title="Contacts" data-bs-toggle="tooltip" data-bs-placement="right">
          <svg class="bi pe-none" width="24" height="24" role="img" aria-label="Customers"><use xlink:href="#people-circle"/></svg>
        </a>
      </li>
  </ul>
    
</nav>
    </div>
</header>

<main>
    
  <section>
<div class="bd-example">
  <div class="row">
    <!--<div class="col-1">
      <div id="list-example" class="list-group">
         <ul class="nav nav-pills nav-flush flex-column mb-auto text-center">
      <li class="nav-item">
        <a href="#rec_act" class="nav-link active py-3 border-bottom rounded-0" aria-current="page" title="Home" data-bs-toggle="tooltip" data-bs-placement="right">
          <svg class="bi pe-none" width="24" height="24" role="img" aria-label="Who I am"><use xlink:href="#person"/></svg>
        </a>
      </li>
      <li>
        <a href="#activity" class="nav-link py-3 border-bottom rounded-0" title="Activity" data-bs-toggle="tooltip" data-bs-placement="right">
          <svg class="bi pe-none" width="24" height="24" role="img" aria-label="Activity"><use xlink:href="#activity"/></svg>
        </a>
      </li>
      <li>
        <a href="#resume" class="nav-link py-3 border-bottom rounded-0" title="Dashboard" data-bs-toggle="tooltip" data-bs-placement="right">
          <svg class="bi pe-none" width="24" height="24" role="img" aria-label="Dashboard"><use xlink:href="#earmark"/></svg>
        </a>
      </li>
      <li>
        <a href="#projects" class="nav-link py-3 border-bottom rounded-0" title="Orders" data-bs-toggle="tooltip" data-bs-placement="right">
          <svg class="bi pe-none" width="24" height="24" role="img" aria-label="Orders"><use xlink:href="#folder"/></svg>
        </a>
      </li>
      <li>
        <a href="#contacts" class="nav-link py-3 border-bottom rounded-0" title="Customers" data-bs-toggle="tooltip" data-bs-placement="right">
          <svg class="bi pe-none" width="24" height="24" role="img" aria-label="Customers"><use xlink:href="#people-circle"/></svg>
        </a>
      </li>
    </ul>
     </div>
    </div>-->
    <div class="col-12 d-flex d-flex justify-content-center" >
        <div class="col-1"></div>
      <div data-bs-spy="scroll" data-bs-target="#list-example" data-bs-offset="0" class="scrollspy-example col-10" tabindex="0">
        <h4 id="personal">Who I am.</h4>
        <br>
        <p> I'm a Computer Vision & Deep Learning Researcher at Leonardo S.p.A. My research interests concern different branches of AI with a particular focus on Vision & Language Tasks. I approached AI in 2018
and during these years I had the occasion of working on a large number of projects involving different AI fields.
          I did my PhD at Media Integration and Communication Center (MICC) at University of Florence, advised by Professor Andrew David Bagdanov and Professor Alberto del Bimbo.
            <br>
            <br>
            My research interests focus on Vision & Language tasks. In particular, I'm interested in systems capable to interact with users taking into consideration visual information like 
            Visual Question Answering.
            Another interest of my research concerns Image Quality Assessment.
            <br>
            <br>
            I studied Computer Engineering at University of Florence where I received my bachelor degree in October 2016 and master Degree in February 2019. Before starting my PhD in November 2019
            I worked as Research Fellow at Media Integration ans Communication Center (MICC).
        </p>
        <br>
        <br>
        At Leonardo I am working on multiple Reasearch and Development projects for internal products (mostly related to Optronics Systems) and external customers involving different fields of study:
<ul>
<li>Event-Based Object Recognition (Spike Neural Networks).</li>
<li>Automatic Metalens Design.</li>
<li>Image super-resolution (CNN, Vision Transformers, Diffusers).</li>
<li>Object Detection.</li>
<li>Document understanding, Document Question Answering (Vision Transformers, LLMs, Knwoledge Graph Generation).</li>
<li>Natural Language Processing (NLP) using Large Language Models (LLMs).</li>
  </ul>
        <br>
        <br>
        <h4 class="col-12 d-flex" id="activity">Recent Activity</h4>
        <br>
        <div class="col-12 d-flex">
            <br>
            <div class="col-2 d-flex">
                <p>Mar 8, 2023</p>
            </div>
             <div class="col-10 d-flex">
                 <p> Our paper "STILT: Scene-Text Image and Language Transformer for Cross-Modal Retrieval" was submitted at ICCV 2023.
               </p>
            </div>
        </div>
        <div class="col-12 d-flex">
            <br>
            <div class="col-2 d-flex">
                <p>Aug 8, 2022</p>
            </div>
             <div class="col-10 d-flex">
                 <p> Our paper "Is GPT-3 is all you need for Visual Question Answering in Cultural Heritage?" was accepted at
            ECCV 2022 Workshop VISART (Computer VISion for ART).
               </p>
        </div>
        </div>
        <div class="col-12 d-flex">
            <div class="col-2 d-flex">
                <p>Aug 5, 2022</p>
            </div>
             <div class="col-10 d-flex">
            <p>Our work entitled "VISCOUNTH: A Large-Scale Visual Question Answering Dataset for Cultural Heritage" was submitted
               at ACM journal Transaction on Multimedia Computing.
            </p>
            </div>
            </div>
        <div class="col-12 d-flex">
            <div class="col-2 d-flex">
                <p>Jul 10, 2022</p>
            </div>
             <div class="col-10 d-flex">
            <p>I will attend the International Computer Vision Summer School (ICVSS) in Sicily for which I was selected.</p>
            </div>
            </div>
        <div class="col-12 d-flex">
            <br>
            <div class="col-2 d-flex">
                <p>Jul 9, 2022</p>
            </div>
             <div class="col-10 d-flex">
                 <p> Our paper "Is GPT-3 is all you need for Visual Question Answering in Cultural Heritage?" was submitted to
            ECCV 2022 Workshop VISART (Computer VISion for ART).
        </div>
        </div>
        <div class="col-12 d-flex">
            <br>
            <div class="col-2 d-flex">
                <p>May 10, 2022</p>
            </div>
             <div class="col-10 d-flex">
                 <p> Our work "LANBIQUE: LANguage-based Blind Image QUality Evaluation" was published at the ACM journal 
            Transaction on Multimedia Computing. The work is an extension of the previous work "Language-Based Image Quality Assessment".</p>
        </div>
        </div>
        <div class="col-12 d-flex">
            <br>
            <div class="col-2 d-flex">
                <p>Dec 5, 2021</p>
            </div>
             <div class="col-10 d-flex">
                 <p> Our paper "Language-Based Image Quality Assessment" was elected Best Paper Award at ACM Multimedia ASIA 2021. </p>
        </div>
        </div>
        <div class="col-12 d-flex">
            <br>
            <div class="col-2 d-flex">
                <p>Oct 15, 2021</p>
            </div>
             <div class="col-10 d-flex">
                 <p> I stated a visiting period of six months at Computer Vision Center in Universitat Autonoma de Barcelona.
            I will work on Cross-Modal retrieval tutored by Professor Dimosthenis Karatstas.</p>
        </div>
        </div>
        <div class="col-12 d-flex">
            <br>
            <div class="col-2 d-flex">
                <p>Jul 5, 2021</p>
            </div>
             <div class="col-10 d-flex">
                 <p> Our paper "Language-Based Image Quality Assessment" was accepted at ACM-Multimedia ASIA 2021 </p>
            </div>
        </div>
        <div class="col-12 d-flex">
            <br>
            <div class="col-2 d-flex">
                <p>Jan 1, 2021</p>
            </div>
             <div class="col-10 d-flex">
                 <p> Our demo paper "Data Collection for Contextual and Visual Question Answering in the Cultural Heritage Domain"
                was acceped and presented at "International Confernce on Pattern Recognition" (ICPR).</p>
            </div>
        </div>
       <div class="col-12 d-flex">
            <br>
            <div class="col-2 d-flex">
                <p>May 5, 2020</p>
            </div>
             <div class="col-10 d-flex">
                 <p> Our work "Visual question answering for cultural heritage" was accepted at the International Conference
            "Florence Heritech".</p>
            </div>
        </div>
        <div class="col-12 d-flex">
            <br>
            <div class="col-2 d-flex">
                <p>Oct 9, 2019</p>
            </div>
             <div class="col-10 d-flex">
                 <p> My thesis work "GADA: Generative adversarial data augmentation for image quality assessment" was accepted at
                "International Conference in Image Analysis and Processing".</p>
            </div>
        </div>
        <div class="col-12 d-flex">
            <br>
            <div class="col-2 d-flex">
                <p>May 1, 2019</p>
            </div>
             <div class="col-10 d-flex">
                 <p> I started to work as Research Fellow at Media Integration and Communication Center (MICC) in 
            University of Florence. My reasearch will focus on Visual Question Answering.</p>
            </div>
        </div>
        <br>
        <br>
            
        
        
        
        
       
        <h4 id="resume">Publications</h4>
        <br>
        <div class="col-12 d-flex">
            <div class="row">
                <div class="col-md-3 flex-column align-items-start">
                    <img src="./images/gpt3_visart.jpg" alt="..." class="img-thumbnail">
                </div>
                <div class="col-md-9 d-flex">
                    <div class="row">
                        <div class="col-md-12 d-flex">
                            <div class="row d-flex">
                                <div>
                                    <p><b> VISCOUNTH: A Large-Scale Multilingual Visual Question Answering Dataset for Cultural Heritage </b></p>
                                    <p>Federico Becattini, Pietro Bongini, Luana Bulla, Alberto Del Bimbo, Ludovica Marinucci, Misael Mongiovì, Valentina Presutti.</p>
                                    <em>ACM Transactions on Multimedia Computing, Communications and Applications.</em>
                                </div>
                                <div>
                                    <p>
                                        <button type="button" class="btn btn-outline-primary" id='button5' onclick="extend(this.id)">Abstract</button>
                                    <a class="btn btn-outline-primary" href="https://dl.acm.org/doi/pdf/10.1145/3590773" target="_blank">Paper</a>
                                    </p>
                                </div>
                                <div class="col-md-12 d-flex border border-primary" id="divpaper5" style="display:none !important">
                                  <p> Visual question answering has recently been settled as a fundamental multi-modal reasoning task of artificial intelligence that allows users to get information about visual content by asking questions in natural language. In the cultural heritage domain this task can contribute to assist visitors in museums and cultural sites, thus increasing engagement. However, the development of visual question answering models for cultural heritage is prevented by the lack of suitable large-scale datasets. To meet this demand, we built a large-scale heterogeneous and multilingual (Italian and English) dataset for cultural heritage that comprises approximately 500K Italian cultural assets and 6.5M question-answer pairs. We propose a novel formulation of the task that requires reasoning over both the visual content and an associated natural language description, and present baselines for this task. Results show that the current state of the art is reasonably effective, but still far from satisfactory, therefore further research is this area is recommended. Nonetheless, we also present a holistic baseline to address visual and contextual questions and foster future research on the topic.</p>
                              </div>
                            </div>
                        </div>
                      </div>
                </div>
                
            </div>
        </div>
            
        <br>
        <div class="col-12 d-flex">
            <div class="row">
                <div class="col-md-3 flex-column align-items-start">
                    <img src="./images/gpt3_visart.jpg" alt="..." class="img-thumbnail">
                </div>
                <div class="col-md-9 d-flex">
                    <div class="row">
                        <div class="col-md-12 d-flex">
                            <div class="row d-flex">
                                <div>
                                    <p><b> Is GPT-3 all you need for Visual Question Answering in Cultural Heritage? </b></p>
                                    <p>Pietro Bongini, Federico Becattini, Alberto Del Bimbo.</p>
                                    <em>ECCV 2022 Workshop VISART (Computer VISion for ART).</em>
                                </div>
                                <div>
                                    <p>
                                        <button type="button" class="btn btn-outline-primary" id='button5' onclick="extend(this.id)">Abstract</button>
                                    <a class="btn btn-outline-primary" href="https://arxiv.org/pdf/2207.12101.pdf" target="_blank">Paper</a>
                                    </p>
                                </div>
                                <div class="col-md-12 d-flex border border-primary" id="divpaper5" style="display:none !important">
                            <p>The use of Deep Learning and Computer Vision in the Cultural Heritage domain is becoming highly relevant in the last few years with lots of applications about audio smart guides, interactive museums and augmented reality. All these technologies require lots of data to work effectively and be useful for the user. In the context of artworks, such data is annotated by experts in an expensive and time consuming process. In particular, for each artwork, an image of the artwork and a description sheet have to be collected in order to perform common tasks like Visual Question Answering. In this paper we propose a method for Visual Question Answering that allows to generate at runtime a description sheet that can be used for answering both visual and contextual questions about the artwork, avoiding completely the image and the annotation process. For this purpose, we investigate on the use of GPT-3 for generating descriptions for artworks analyzing the quality of generated descriptions through captioning metrics. Finally we evaluate the performance for Visual Question Answering and captioning tasks.</p>
                        </div>
                            </div>
                        </div>
                      </div>
                </div>
                
            </div>
        </div>
        <br>        
                      
                      
        <div class="col-12 d-flex">
            <div class="row">
                <div class="col-md-3 flex-column align-items-start">
                    <img src="./images/LANBIQUE_NOCAP-crop.jpg" alt="..." class="img-thumbnail">
                </div>
                <div class="col-md-9 d-flex">
                    <div class="row">
                        <div class="col-md-12 d-flex">
                            <div class="row d-flex">
                                <div>
                                    <p><b> LANBIQUE: LANguage-based Blind Image QUality Evaluation.</b></p>
                                    <p>Leonardo Galteri, Lorenzo Seidenari, Pietro Bongini, Marco Bertini, Alberto Del Bimbo.</p>
                                    <em>ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM).</em>
                                </div>
                                <div>
                                    <p>
                                        <button type="button" class="btn btn-outline-primary" id='button5' onclick="extend(this.id)">Abstract</button>
                                    <a class="btn btn-outline-primary" href="https://dl.acm.org/doi/abs/10.1145/3538649" target="_blank">Paper</a>
                                    </p>
                                </div>
                                <div class="col-md-12 d-flex border border-primary" id="divpaper5" style="display:none !important">
                            <p>Image quality assessment is often performed with deep networks which are fine-tuned to regress a human provided quality score of a given image. Usually, this approaches may lack generalization capabilities and, while being highly precise on similar image distribution, it may yield lower correlation on unseen distortions. In particular they show poor performances whereas images corrupted by noise, blur or compressed have been restored by generative models. As a matter of fact, evaluation of these generative models is often performed providing anecdotal results to the reader. In the case of image enhancement and restoration, reference images are usually available. Nonetheless, using signal based metrics often leads to counterintuitive results: highly natural crisp images may obtain worse scores than blurry ones. On the other hand, blind reference image assessment may rank images reconstructed with GANs higher than the original undistorted images. To avoid time consuming human based image assessment, semantic computer vision tasks may be exploited instead.
In this paper we advocate the use of language generation tasks to evaluate the quality of restored images. We refer to our assessment approach as LANguage-based Blind Image QUality Evaluation (LANBIQUE). We show experimentally that image captioning, used as a downstream task, may serve as a method to score image quality, independently of the distortion process that affects the data. Captioning scores are better aligned with human rankings with respect to classic signal based or No-Reference image quality metrics. We show insights on how the corruption, by artifacts, of local image structure may steer image captions in the wrong direction.</p>
                        </div>
                            </div>
                        </div>
                 
               
                        <!--<div class="col-9 d-flex">
                            <p><button type="button" class="btn btn-outline-primary" id='button5' onclick="extend(this.id)">Abstract</button>
                            <a class="btn btn-outline-primary" href="https://dl.acm.org/doi/abs/10.1145/3538649" target="_blank">Paper</a>
                            </p>
                        </div>
                        <div class="col-md-12 d-flex border border-primary" id="divpaper5" style="display:none !important">
                            <p>Image quality assessment is often performed with deep networks which are fine-tuned to regress a human provided quality score of a given image. Usually, this approaches may lack generalization capabilities and, while being highly precise on similar image distribution, it may yield lower correlation on unseen distortions. In particular they show poor performances whereas images corrupted by noise, blur or compressed have been restored by generative models. As a matter of fact, evaluation of these generative models is often performed providing anecdotal results to the reader. In the case of image enhancement and restoration, reference images are usually available. Nonetheless, using signal based metrics often leads to counterintuitive results: highly natural crisp images may obtain worse scores than blurry ones. On the other hand, blind reference image assessment may rank images reconstructed with GANs higher than the original undistorted images. To avoid time consuming human based image assessment, semantic computer vision tasks may be exploited instead.
In this paper we advocate the use of language generation tasks to evaluate the quality of restored images. We refer to our assessment approach as LANguage-based Blind Image QUality Evaluation (LANBIQUE). We show experimentally that image captioning, used as a downstream task, may serve as a method to score image quality, independently of the distortion process that affects the data. Captioning scores are better aligned with human rankings with respect to classic signal based or No-Reference image quality metrics. We show insights on how the corruption, by artifacts, of local image structure may steer image captions in the wrong direction.</p>
                        </div>-->
                    </div>
                </div>
                
            </div>
        </div>
        <br>
         <div class="col-12 d-flex">
            <div class="row">
                <div class="col-md-3 flex-column align-items-start">
                    <img src="./images/drawing.jpg" alt="..." class="img-thumbnail">
                </div>
                <div class="col-md-9 d-flex">
                    <div class="row">
                        <div class="col-md-12 d-flex">
                            <div class="row d-flex">
                                <div>
                                <div><p><b>Language based image quality assessment.</b></p>
                                    <p>Lorenzo Seidenari, Leonardo Galteri, Pietro Bongini, Marco Bertini, Alberto Del Bimbo</p>
                                    <em>In ACM Multimedia Asia (pp. 1-7). </em>
                                </div>
                 
               
                                <div>
                                    <p><button type="button" class="btn btn-outline-primary" id='button4' onclick="extend(this.id)">Abstract</button>
                                    <a class="btn btn-outline-primary" href="https://dl.acm.org/doi/abs/10.1145/3469877.3490605" target="_blank">Paper</a>
                                    <!--<button type="button" class="btn btn-outline-primary">Poster</button>-->
                                    <a class="btn btn-outline-primary" href="https://docs.google.com/presentation/d/1ovi2tLn9640DGz_tiFn03bvQpWSZfkvm/edit?usp=sharing&ouid=101453933625655268839&rtpof=true&sd=true" target="_blank">Slides</a>
                                    <!--<button type="button" class="btn btn-outline-primary">Presentation</button>-->
                                    </p>                        
                                </div>
                                <div class="col-md-12 d-flex border border-primary" id="divpaper4" style="display:none !important">
                                    <p>Evaluation of generative models, in the visual domain, is often performed providing anecdotal results to the reader. In the case of image enhancement, reference images are usually available. Nonetheless, using signal based metrics often leads to counterintuitive results: highly natural crisp images may obtain worse scores than blurry ones. On the other hand, blind reference image assessment may rank images reconstructed with GANs higher than the original undistorted images. To avoid time consuming human based image assessment, semantic computer vision tasks may be exploited instead. In this paper we advocate the use of language generation tasks to evaluate the quality of restored images. We show experimentally that image captioning, used as a downstream task, may serve as a method to score image quality. Captioning scores are better aligned with human rankings with respect to signal based metrics or no-reference image quality metrics. We show insights on how the corruption, by artifacts, of local image structure may steer image captions in the wrong direction.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                    </div>
                </div>
            </div>
         </div>
        <br>
        <div class="col-12 d-flex">
            <div class="row">
                <div class="col-md-3 flex-column align-items-start">
                    <img src="./images/icpr.png" alt="..." class="img-thumbnail">
                </div>
                <div class="col-md-9 d-flex">
                    <div class="row">
                        <div class="col-md-12 d-flex">
                            <div class="row d-flex">
                                <div>
                                <div>
                                    <p><b>Data Collection for Contextual and Visual Question Answering in the Cultural Heritage Domain. </b></p>
                                    <p>Francesco Vannoni Pietro Bongini Federico Becattini, Andrew David Bagdanov, Alberto Del Bimbo</p>
                              
                                </div>
                 
               
                                <div class="col-12 d-flex">
                                    <p><button type="button" class="btn btn-outline-primary" id='button3' onclick="extend(this.id)">Abstract</button>
                                    <a class="btn btn-outline-primary" href="https://www.micc.unifi.it/icpr2020/wp-content/uploads/demos/s4.6-paper.pdf" target="_blank">Paper</a>
                                    <a class="btn btn-outline-primary" href="https://drive.google.com/file/d/1uh9eKe5F5j2fuxYsrjMiv-uk024J-YJZ/view?usp=sharing" target="_blank">Demo</a>
                                    </p>                        
                                </div>
                                <div class="col-md-12 d-flex border border-primary" id="divpaper3" style="display:none !important">
                                    <p>In this demonstration we propose an annotation tool to collect question-answer samples for artworks, necessary to train and evaluate visual and contextual question answering models. The tool is completely web-based, and relies on an automatic question-answer generation model to aid the annotation process. Through the annotator, users can inspect and refine the generated annotations and obtain statistics on their quality. A pre-trained visual and contextual question answering model is also provided to the final user to be able to interact with the system by asking questions about artworks.</p>
                                </div>
                            </div>
                        </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <br>
        <div class="col-12 d-flex">
            <div class="row">
                <div class="col-md-3 flex-column align-items-start">
                    <img src="./images/vqamodel.jpg" alt="..." class="img-thumbnail">
                </div>
                <div class="col-md-9 d-flex">
                    <div class="row">
                        <div class="col-md-12 d-flex">
                            <div class="row d-flex">
                                <div>
                                <div>
                                        <p><b> Visual question answering for cultural heritage. </b></p> 
                                        <p>Pietro Bongini, Federico Becattini, Andrew D Bagdanov, Alberto Del Bimbo </p> 
                                        <em>In IOP Conference Series: Materials Science and Engineering. IOP Publishing. </em>
                                    </div>
                 
               
                                    <div class="col-12 d-flex">
                                        <p><button type="button" class="btn btn-outline-primary" id='button2' onclick="extend(this.id)">Abstract</button>
                                        <a class="btn btn-outline-primary" href="https://iopscience.iop.org/article/10.1088/1757-899X/949/1/012074/pdf" target="_blank">Paper</a>
                                        <a class="btn btn-outline-primary" href="https://docs.google.com/presentation/d/1V1eMff0r93IUzojlcy06sepQ4ET0m9t1_2IhDZRw48A/edit?usp=sharing" target="_blank">Slides</a>
                                        <a class="btn btn-outline-primary" href="https://www.youtube.com/watch?v=RJEVVIDsHH4&t=5s" target="_blank">Presentation</a>
                                        </p>                        
                                    </div>
                                    <div class="col-md-12 d-flex border border-primary" id="divpaper2" style="display:none !important">
                                        <p>Technology and the fruition of cultural heritage are becoming increasingly more entwined, especially with the advent of smart audio guides, virtual and augmented reality, and interactive installations. Machine learning and computer vision are important components of this ongoing integration, enabling new interaction modalities between user and museum. Nonetheless, the most frequent way of interacting with paintings and statues still remains taking pictures. Yet images alone can only convey the aesthetics of the artwork, lacking is information which is often required to fully understand and appreciate it. Usually this additional knowledge comes both from the artwork itself (and therefore the image depicting it) and from an external source of knowledge, such as an information sheet. While the former can be inferred by computer vision algorithms, the latter needs more structured data to pair visual content with relevant information. Regardless of its source, this information still must be be effectively transmitted to the user. A popular emerging trend in computer vision is Visual Question Answering (VQA), in which users can interact with a neural network by posing questions in natural language and receiving answers about the visual content. We believe that this will be the evolution of smart audio guides for museum visits and simple image browsing on personal smartphones. This will turn the classic audio guide into a smart personal instructor with which the visitor can interact by asking for explanations focused on specific interests. The advantages are twofold: on the one hand the cognitive burden of the visitor will decrease, limiting the flow of information to what the user actually wants to hear; and on the other hand it proposes the most natural way of interacting with a guide, favoring engagement.</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
       
        
       
        <br>
          <div class="col-12 d-flex">
            <div class="row">
                <div class="col-md-3 flex-column align-items-start">
                    <img src="./images/icpr.png" alt="..." class="img-thumbnail">
                </div>
                <div class="col-md-9 d-flex">
                    <div class="row">
                        <div class="col-md-12 d-flex">
                            <div class="row d-flex">
                                <div>
                                <div>
                                    <p><b> GADA: Generative adversarial data augmentation for image quality assessment. </b></p>
                                    <p>Pietro Bongini, Riccardo Del Chiaro, Andrew D Bagdanov, Alberto Del Bimbo.</p>
                                    <em>In International Conference on Image Analysis and Processing (pp. 214-224). Springer, Cham.</em>
                                </div>
                 
               
                                <div class="col-12 d-flex">
                                    <p><button type="button" class="btn btn-outline-primary" id='button1' onclick="extend(this.id)">Abstract</button>
                                    <a class="btn btn-outline-primary" href="https://link.springer.com/chapter/10.1007/978-3-030-30645-8_20" target="_blank">Paper</a>
                                    <a class="btn btn-outline-primary" href="https://drive.google.com/file/d/1BZx7ZUbkwfcVnkZsI7p753i6tB3PsLvZ/view?usp=sharing" target="_blank">Poster</a>
                                    <a class="btn btn-outline-primary" href="https://docs.google.com/presentation/d/1WDmQcr561hDNm_Xw7cTxZCHhpksK2Gq4/edit?usp=sharing&ouid=101453933625655268839&rtpof=true&sd=true" target="_blank">Slides</a>
                                    </p>                        
                                </div>
                                <div class="col-md-12 d-flex border border-primary" id="divpaper1" style="display:none !important">
                                    <p>We propose a No-reference Image Quality Assessment (NR-IQA) approach based on the use of generative adversarial networks. To address the problem of lack of adequate amounts of labeled training data for NR-IQA, we train an Auxiliary Classifier Generative Adversarial Network (AC-GAN) to generate distorted images with various distortion types and levels of image quality at training time. The trained generative model allow us to augment the size of the training dataset by introducing distorted images for which no ground truth is available. We call our approach Generative Adversarial Data Augmentation (GADA) and experimental results on the LIVE and TID2013 datasets show that our approach – using a modestly sized and very shallow network – performs comparably to state-of-the-art methods for NR-IQA which use significantly more complex models. Moreover, our network can process images in real time at 120 image per second unlike other state-of-the-art techniques.</p>
                                </div>
                            </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            </div>
 
        <!--<h4 id="projects">Item 3</h4>
        <p>This is some placeholder content for the scrollspy page. Note that as you scroll down the page, the appropriate navigation link is highlighted. It's repeated throughout the component example. We keep adding some more example copy here to emphasize the scrolling and highlighting.</p>
        -->
        <h4 id="contacts">Contacts</h4>
        <nav>
            <ul class="nav nav-pills">
              <li class="nav-item">
                  <a href="mailto:p.bongini@unifi.it" target="_blank" class="nav-link active py-3 rounded-0" aria-current="page" title="Home" data-bs-toggle="tooltip" data-bs-placement="right">
                    <svg class="bi pe-none" width="24" height="24" role="img" aria-label="E-mail"><use xlink:href="#email"/></svg>
                  </a>
              </li>
              <li class="nav-item">
               <a href="https://www.linkedin.com/in/pietrobongini/" target="_blank" class="nav-link py-3  rounded-0" title="Linkedin" data-bs-toggle="tooltip" data-bs-placement="right">
                   <svg class="bi pe-none" width="24" height="24" role="img" aria-label="linkedin"><use xlink:href="#linkedin"/></svg>
                  </a>
              </li>
            </ul>
        </
      </div>
        <div class="col-1"></div>
    </div>
  </div>
</div>
  </section>
</main>

<footer class="text-muted py-5">
  <div class="container">
    <p class="float-end mb-1">
      <a class="btn btn-outline-primary" href="#personal">Back to top</a>
    </p>
    <p class="mb-1"> &copy; Copyright 2022 Pietro Bongini all rights reserved</p>
    
  </div>
</footer>


    <script src="../assets/dist/js/bootstrap.bundle.min.js"></script>

      
  </body>
</html>
